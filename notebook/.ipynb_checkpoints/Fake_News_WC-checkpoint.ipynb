{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e5cf57",
   "metadata": {},
   "source": [
    "Detecting Deceit - Training the model to predict when there is fake news on social media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a59dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffc789",
   "metadata": {},
   "source": [
    "Reading in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7500ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_excel('../data/raw/News_Detection_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0f910",
   "metadata": {},
   "source": [
    "Removing any records that are blank from the dataset and reset the index for further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a140247",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.dropna(inplace = True)\n",
    "\n",
    "news_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326eaffa",
   "metadata": {},
   "source": [
    "Removing numbers, brackets and invalid characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b78424",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['Title'] = news_df['Title'].str.replace('\\d+', '', regex=True)\n",
    "news_df['Title'] = news_df['Title'].str.replace(r'\\(|\\)', '', regex=True)\n",
    "news_df['Title'] = news_df['Title'].str.replace(\"â€™\", \"\", regex=True)\n",
    "news_df['Title'] = news_df['Title'].str.replace(\"Ã¢\", \"\", regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01adb17c",
   "metadata": {},
   "source": [
    "Function to be used to remove punctation from the tweets then applying the function to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861d9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "news_df['Title']= news_df['Title'].apply(lambda x:remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13f801",
   "metadata": {},
   "source": [
    "Using a function to tokenize the text then applying the function to the data and creating a new series in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87bd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_words(text):\n",
    "    words = re.split(r'\\W+',text)\n",
    "    return words\n",
    "\n",
    "\n",
    "news_df['Title_Tokenized']= news_df['Title'].apply(lambda x: tokenize_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ed1e1",
   "metadata": {},
   "source": [
    "Lowering the case of the characters for the purposes of removing the stopwords due to case sensitivity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8f2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_df['Title_Tokenized'])):\n",
    "    news_df['Title_Tokenized'][i] = [token.lower() for token in news_df['Title_Tokenized'][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21ddde",
   "metadata": {},
   "source": [
    "Removal of Stop Words and completing text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2deb677a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\User\n",
      "[nltk_data]     1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemma_word_list = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "\n",
    "for tokenized_data in news_df['Title_Tokenized']:\n",
    "    filtered_data = [x for x in tokenized_data if x.lower() not in stop_words]\n",
    "    \n",
    "    lemma_words = []\n",
    "\n",
    "    for w in filtered_data:\n",
    "        word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "        word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "        word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "        lemma_words.append(word3)\n",
    "    lemma_word_list.append(lemma_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f23cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"Title_Text\"] = lemma_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370c843",
   "metadata": {},
   "source": [
    "Removing the columns that will not be part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8d15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.drop(['Title_Tokenized', 'Title'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16912568",
   "metadata": {},
   "source": [
    "OHE (one hot encoding) for the labelling if the tweet from GossipCop is real or fake news. 1 will identify the news that is real and 0 will identify the fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4cfdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['Label'] = pd.get_dummies(news_df['Label'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff23fcd",
   "metadata": {},
   "source": [
    "Creating a copy of the dataframe and renaming it for the training and testing of the model. Also, saving a copy of the dataframe that will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001173b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_News_Data = news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dddc469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_News_Data.to_excel(\"../data/processed/Processed_News_Data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff305577",
   "metadata": {},
   "source": [
    "Splitting the data for training with 70% of the data, and testing will be completed with 30%. Saving the separated training and testing data as their own datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a36218",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(Processed_News_Data, test_size=0.3, random_state=101)\n",
    "\n",
    "train_df.to_csv('../data/raw/train.csv', index=False)\n",
    "test_df.to_csv('../data/raw/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0fcb4",
   "metadata": {},
   "source": [
    "Brining the training data into the workbook for the purposes of training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0de6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc37ae",
   "metadata": {},
   "source": [
    "Creation of the Tfidf Vectorizer instance, then using the fit which will convert to numereical format so it can be used in the model. Then transform which will create the Tfidf matrix based on the what was learned in the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50e5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(training_data['Title_Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0fd8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../models/tf_processor.pkl\"\n",
    "with open(filename, 'wb') as vectorizer_file:\n",
    "    pickle.dump(tfidf_vectorizer, vectorizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1bdd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44850150",
   "metadata": {},
   "source": [
    "Parameteres for SVM and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7497c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC()\n",
    "svm_param_grid ={\n",
    "    \"C\":[0.1,1,10],\n",
    "    \"kernel\":['linear'],\n",
    "    \"gamma\":['scale', 'auto']\n",
    "    \n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_param_grid ={\n",
    "    \"n_estimators\":[25,50,75],\n",
    "    \"max_depth\":[None,5],\n",
    "    \"min_samples_split\":[2,5,10],\n",
    "    \"min_samples_leaf\":[1,2,4],\n",
    "    \"class_weight\": [\"balanced\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e39be0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPPORT VECTOR MACHINE, RANDOM FOREST WITH GRID SEARCH\n",
    "\n",
    "svm_grid_search = GridSearchCV(svm_model, param_grid=svm_param_grid, cv=4, n_jobs=-1)\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_model, param_grid=rf_param_grid, cv=5, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_search.fit(X_train_tfidf,y_train)   \n",
    "rf_grid_search.fit(X_train_tfidf,y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1baad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_model = svm_grid_search.best_estimator_\n",
    "rf_best_model = rf_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ff3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ensemble(X):\n",
    "    svm_preds= svm_best_model.predict(X)\n",
    "    rf_preds= rf_best_model.predict(X)\n",
    "    ensemble_preds= [1 if (svm_pred + rf_pred) >= 1 \n",
    "                     else 0 for svm_pred, rf_pred in zip(svm_preds, rf_preds)]\n",
    "    return ensemble_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8559434",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_test_pred = ensemble_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec029a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc2409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"../models/svm_model.pkl\"\n",
    "pickle.dump(svm_model,open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "filename = \"../models/grid_search_processor.pkl\"\n",
    "pickle.dump(grid_search,open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9800ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "\n",
    "best_parameters= grid_search.best_params_\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "test_preds = best_model.predict(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada18845",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_train = best_model.predict(X_train_tfidf)\n",
    "\n",
    "print(classification_report(y_train,y_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a98dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_train, test_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = pd.DataFrame({'Predictions': y_preds_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions.to_excel(\"../results/training_prediction.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cf697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07402825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b31bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0451e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1a670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f03dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST WITH GRID SEARCH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
